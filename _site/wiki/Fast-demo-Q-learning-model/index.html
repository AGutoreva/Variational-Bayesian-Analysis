<!DOCTYPE html>
<html>

      <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>VBA toolbox</title>
        <meta name="viewport" content="width=device-width">
        <meta name="description" content="Write an awesome description for your new site here. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
        <link rel="canonical" href="/wiki/Fast-demo-Q-learning-model//" />

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">
        <!-- <link rel="stylesheet" href="/css/pygments_default.css">-->

<!-- icon font -->
        <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!--<script type="text/javascript" src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript" src="/js/jQuery.js"></script>
<script type="text/javascript" src="/js/toc.js"></script>-->
<script type="text/javascript">
$(document).ready(function() {
    $('.toc').toc();
});
</script>

<!--<script type="text/javascript" src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->

    </head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <a class="site-title" href="/">VBA toolbox</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        <a class="page-link" href="/install"><i class="fa fa-download"></i> Install</a>
        <a class="page-link" href="/wiki"><i class="fa fa-book"></i> Wiki</a>
        <a class="page-link" href="/about"><i class="fa fa-mortar-board"></i> About</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">
  <header class="post-header">
  <h1 id="title"><a href="/wiki">WIKI</a>&nbsp;Fast demo Q learning model</h1>
 </header>

  <article class="post-content">
    <hr/>
  
<p>Below, we copied and pasted the content and graphical output of a demonstration script (<code>demo_Qlearning.m</code>) that replicates a typical learning study in behavioural economics and/or experimental psychology.<br />
The task is the so-called two-armed bandit problem, which captures the essence of operant learning. The demo uses the so-called <a href="/wiki/Structure-of-VBA's-generative-model">Q-learning model</a> which predicts how people change their behavioural response according to the consequences of their actions (e.g., reward/punishment feedback).</p>

<div><div class="CodeRay">
  <div class="code"><pre><span class="line-numbers"> <a href="#n1" name="n1">1</a></span>% Q-learning demo
<span class="line-numbers"> <a href="#n2" name="n2">2</a></span>% In psychological terms, motivation can be defined as the set of processes
<span class="line-numbers"> <a href="#n3" name="n3">3</a></span>% that generate goals and thus determine behaviour. A goal is nothing else
<span class="line-numbers"> <a href="#n4" name="n4">4</a></span>% than a “state of affairs”, to which people attribute (subjective) value.
<span class="line-numbers"> <a href="#n5" name="n5">5</a></span>% Empirically speaking, one can access these values by many means,
<span class="line-numbers"> <a href="#n6" name="n6">6</a></span>% including subjective verbal report or decision making. These measures
<span class="line-numbers"> <a href="#n7" name="n7">7</a></span>% have been used to demonstrate how value change as people learn a new
<span class="line-numbers"> <a href="#n8" name="n8">8</a></span>% operant response. This is predicted by reinforcement learning theories,
<span class="line-numbers"> <a href="#n9" name="n9">9</a></span>% which essentially relate behavioural response frequency to reward. In
<span class="line-numbers"><strong><a href="#n10" name="n10">10</a></strong></span>% this context, value is expected reward, and it changes in proportion to
<span class="line-numbers"><a href="#n11" name="n11">11</a></span>% the agent prediction error, i.e. the difference between actual and
<span class="line-numbers"><a href="#n12" name="n12">12</a></span>% expected reward.
<span class="line-numbers"><a href="#n13" name="n13">13</a></span>% This demo simulates a number of sequences of choices of a Q-learning
<span class="line-numbers"><a href="#n14" name="n14">14</a></span>% agent, which is a simple example of reinforcement learning algiorithms.
<span class="line-numbers"><a href="#n15" name="n15">15</a></span>% We then invert the model using VBA. Finally, we perform a Volterra
<span class="line-numbers"><a href="#n16" name="n16">16</a></span>% decomposition of hidden states dynamics onto a set of appropriately
<span class="line-numbers"><a href="#n17" name="n17">17</a></span>% chosen basis functions (here: the agent chosen action, and the winning
<span class="line-numbers"><a href="#n18" name="n18">18</a></span>% action). This diagnostic analysis allows one to identify the hidden
<span class="line-numbers"><a href="#n19" name="n19">19</a></span>% states impulse response to experimentally controlled inputs to the
<span class="line-numbers"><strong><a href="#n20" name="n20">20</a></strong></span>% system.
<span class="line-numbers"><a href="#n21" name="n21">21</a></span>
<span class="line-numbers"><a href="#n22" name="n22">22</a></span>close all
<span class="line-numbers"><a href="#n23" name="n23">23</a></span>clear variables
<span class="line-numbers"><a href="#n24" name="n24">24</a></span>clc
<span class="line-numbers"><a href="#n25" name="n25">25</a></span>
<span class="line-numbers"><a href="#n26" name="n26">26</a></span>
<span class="line-numbers"><a href="#n27" name="n27">27</a></span>f_fname = @f_Qlearn2; % evolution function (Q-learning)
<span class="line-numbers"><a href="#n28" name="n28">28</a></span>g_fname = @g_softmax; % observation function (softmax mapping)
<span class="line-numbers"><a href="#n29" name="n29">29</a></span>h_fname = @h_randOutcome; % feedback function (reward schedule)
<span class="line-numbers"><strong><a href="#n30" name="n30">30</a></strong></span>
<span class="line-numbers"><a href="#n31" name="n31">31</a></span>% allocate feedback struture for simulations
<span class="line-numbers"><a href="#n32" name="n32">32</a></span>fb.inH.er = 1;
<span class="line-numbers"><a href="#n33" name="n33">33</a></span>fb.inH.vr = 0;
<span class="line-numbers"><a href="#n34" name="n34">34</a></span>fb.h_fname = h_fname;
<span class="line-numbers"><a href="#n35" name="n35">35</a></span>fb.indy = 1;
<span class="line-numbers"><a href="#n36" name="n36">36</a></span>fb.indfb = 2;
<span class="line-numbers"><a href="#n37" name="n37">37</a></span>u0 = [randn(1,25)&gt;-0.25]; % possible feedbacks
<span class="line-numbers"><a href="#n38" name="n38">38</a></span>fb.inH.u0 = [u0,~u0,u0,~u0,u0,~u0]; % with reversals
<span class="line-numbers"><a href="#n39" name="n39">39</a></span>
<span class="line-numbers"><strong><a href="#n40" name="n40">40</a></strong></span>% simulation parameters
<span class="line-numbers"><a href="#n41" name="n41">41</a></span>theta = sigm(0.75,struct('INV',1)); % learning rate = 0.75
<span class="line-numbers"><a href="#n42" name="n42">42</a></span>phi = log(2); % inverse temperature = 2
<span class="line-numbers"><a href="#n43" name="n43">43</a></span>x0 = zeros(2,1);
<span class="line-numbers"><a href="#n44" name="n44">44</a></span>n_t = size(fb.inH.u0,2)+1; % number of trials
<span class="line-numbers"><a href="#n45" name="n45">45</a></span>options.binomial = 1;
<span class="line-numbers"><a href="#n46" name="n46">46</a></span>options.verbose = 0;
<span class="line-numbers"><a href="#n47" name="n47">47</a></span>options.skipf = zeros(1,n_t);
<span class="line-numbers"><a href="#n48" name="n48">48</a></span>options.skipf(1) = 1; % apply identity mapping from x0 to x1.
<span class="line-numbers"><a href="#n49" name="n49">49</a></span>
<span class="line-numbers"><strong><a href="#n50" name="n50">50</a></strong></span>% simulate Q-learner and plot choices
<span class="line-numbers"><a href="#n51" name="n51">51</a></span>[y,x,x0,eta,e,u] = simulateNLSS_fb(n_t,f_fname,g_fname,theta,phi,zeros(2,n_t),Inf,Inf,options,x0,fb);
<span class="line-numbers"><a href="#n52" name="n52">52</a></span>hf = figure('color',[1 1 1]);
<span class="line-numbers"><a href="#n53" name="n53">53</a></span>ha = axes('parent',hf,'nextplot','add');
<span class="line-numbers"><a href="#n54" name="n54">54</a></span>plot(ha,y,'kx')
<span class="line-numbers"><a href="#n55" name="n55">55</a></span>plot(ha,y-e,'r')
<span class="line-numbers"><a href="#n56" name="n56">56</a></span>legend(ha,{'y: agent''s choices','p(y=1|theta,phi,m): behavioural tendency'})
<span class="line-numbers"><a href="#n57" name="n57">57</a></span>
<span class="line-numbers"><a href="#n58" name="n58">58</a></span>
<span class="line-numbers"><a href="#n59" name="n59">59</a></span>% VBA model inversion (given simulated choices)
<span class="line-numbers"><strong><a href="#n60" name="n60">60</a></strong></span>dim = struct('n',2,'n_theta',1,'n_phi',1);
<span class="line-numbers"><a href="#n61" name="n61">61</a></span>priors.a_alpha = Inf;
<span class="line-numbers"><a href="#n62" name="n62">62</a></span>priors.b_alpha = 0;
<span class="line-numbers"><a href="#n63" name="n63">63</a></span>options.priors = priors;
<span class="line-numbers"><a href="#n64" name="n64">64</a></span>[posterior,out] = VBA_NLStateSpaceModel(y,u,f_fname,g_fname,dim,options);
<span class="line-numbers"><a href="#n65" name="n65">65</a></span>
<span class="line-numbers"><a href="#n66" name="n66">66</a></span>% compare simulated and estimated model variables
<span class="line-numbers"><a href="#n67" name="n67">67</a></span>displayResults(posterior,out,y,x,x0,theta,phi,Inf,Inf);
<span class="line-numbers"><a href="#n68" name="n68">68</a></span>
<span class="line-numbers"><a href="#n69" name="n69">69</a></span>
<span class="line-numbers"><strong><a href="#n70" name="n70">70</a></strong></span>% perform Volterra decomposition
<span class="line-numbers"><a href="#n71" name="n71">71</a></span>u1 = u(1,:); % own action
<span class="line-numbers"><a href="#n72" name="n72">72</a></span>u3 = u(2,:); % feedback
<span class="line-numbers"><a href="#n73" name="n73">73</a></span>u2 = zeros(size(u1)); % opponent action
<span class="line-numbers"><a href="#n74" name="n74">74</a></span>u2(u3&gt;0) = u1(u3&gt;0);
<span class="line-numbers"><a href="#n75" name="n75">75</a></span>u2(u3&lt;0) = 1-u1(u3&lt;0);
<span class="line-numbers"><a href="#n76" name="n76">76</a></span>uu = 2*[u1;u2]-1;
<span class="line-numbers"><a href="#n77" name="n77">77</a></span>o = out;
<span class="line-numbers"><a href="#n78" name="n78">78</a></span>o.u = uu;
<span class="line-numbers"><a href="#n79" name="n79">79</a></span>[kernels] = VBA_VolterraKernels(posterior,o,16);
<span class="line-numbers"><strong><a href="#n80" name="n80">80</a></strong></span>o.diagnostics.kernels = kernels;
<span class="line-numbers"><a href="#n81" name="n81">81</a></span>VBA_ReDisplay(posterior,o,1)
<span class="line-numbers"><a href="#n82" name="n82">82</a></span>
<span class="line-numbers"><a href="#n83" name="n83">83</a></span>getSubplots
</pre></div>
</div>
</div>

<p>Below are graphical outputs of the demonstration script. First, let us focus on the simulated behavioural tendency and agent’s choices:</p>

<p><img src="/images/wiki/demo1/demo1_1.jpg" alt="" /></p>

<p>On can see how the agent’s behavioural response changes according to the feedback he receives. In brief, the Q-learner is tracking the winning option (which effectively varies over time).</p>

<p>Graphical outputs (parameter estimation, model accuracy, inversion diagnostics, convergence, etc…) of the VBA model inversion are described <a href="VBA-graphical-output.html">elsewhere</a> goes through all of them for this demo). Now let us check how accurate the model inversion was:</p>

<p><img src="/images/wiki/demo1/demo1_2.jpg" alt="" /></p>

<p>Recall the unknown parameters of the <a href="Structure-of-VBA's-generative-model">Q-learning model</a>: 1 evolution parameter (learning rate), 1 observation parameter (temperature) and 2 initial conditions (initial action values).<br />
One can see that the posterior credible intervals (red errorbars) contain the simulated parameter values (green dots). In turn, estimated and simulated action values (as well as choices) dynamics tightly correlate with each other.</p>

<p>Let us now eyeball the Volterra decomposition of action values and choices (w.r.t. winning action):</p>

<p><img src="/images/wiki/demo1/demo1_2.jpg" alt="" /></p>

<p>The ‘winning action’ basis function was encoded as follows: it was 1 when the winning action was the first action, and -1 otherwise. It follows that the first (resp. second) Q-learner’s action value exhibits a positive (resp. negative) exponentially decaying impulse response to the winning action. The decay rate of the Volterra kernel is simply controlled by the learning rate. Note that the choice is modelled as a binomial variable, whose log-odds is proportional to the difference between the first and second action values. This is why the observables’ Volterra kernel is a positive exponentially decaying function of past winning actions.</p>

 <hr/>
  <div class='return_link'><a href="#title"><i class="fa fa-chevron-up"></i> back to top</a></div>
  </article>


</div>


      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">


    <div class="footer-col-1 column">
        <h2 class="footer-heading">Participate to the VBA-toolbox!</h2>

          <ul>
        <li>
           <i class="fa fa-github-square fa-fw "></i>
          <a href="https://github.com/MBB-team/VBA-toolbox/fork">fork on Github</a>
        </li>
        <li>
          <i class="fa fa-edit fa-fw "></i>
          <a href="https://github.com/MBB-team/VBA-toolbox/issues">post a request</a>
        </li>
        </ul>

    </div>

    <div class="footer-col-2 column">
     <ul>
        <li>Jean DAUNIZEAU</li>
        <li><a href="mailto:jean.daunizeau@gmail.com">jean.daunizeau[at]gmail.com</a></li>
      </ul>
      <ul>
        <li>Lionel RIGOUX</li>
        <li><a href="mailto:lionel.rigoux@gmail.com">lionel.rigoux[at]gmail.com</a></li>
      </ul>

     </div>

    <div class="footer-col-3 column">
<p>J. Daunizeau, V. Adam, L. Rigoux (2014), VBA: a probabilistic treatment of nonlinear models for neurobiological and behavioural data. PLoS Comp Biol 10(1): e1003441.</p>
    </div>

  </div>

</footer>


    </body>

</html>
